{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54a38fc7-a550-4bd5-a284-04313d821d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a0b90f0-3b7f-4155-be85-eb0006783f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "names = []\n",
    "with open('names.txt', 'r') as file:\n",
    "    while line := file.readline():\n",
    "        names.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c850b5e-77a7-43a6-82ba-8dbbc50aceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique chars in dataset\n",
    "chars = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        chars.add(c)\n",
    "\n",
    "# Assign value to each character -- this will be our embedding.\n",
    "stoi = {ch:i+1 for i,ch in enumerate(sorted(chars))}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ece6d689-d770-4c6a-8fa2-80dca487124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: emma\n",
      "... --> e\n",
      "..e --> m\n",
      ".em --> m\n",
      "emm --> a\n",
      "mma --> .\n",
      "Name: olivia\n",
      "... --> o\n",
      "..o --> l\n",
      ".ol --> i\n",
      "oli --> v\n",
      "liv --> i\n",
      "ivi --> a\n",
      "via --> .\n",
      "Name: ava\n",
      "... --> a\n",
      "..a --> v\n",
      ".av --> a\n",
      "ava --> .\n",
      "Name: isabella\n",
      "... --> i\n",
      "..i --> s\n",
      ".is --> a\n",
      "isa --> b\n",
      "sab --> e\n",
      "abe --> l\n",
      "bel --> l\n",
      "ell --> a\n",
      "lla --> .\n",
      "Name: sophia\n",
      "... --> s\n",
      "..s --> o\n",
      ".so --> p\n",
      "sop --> h\n",
      "oph --> i\n",
      "phi --> a\n",
      "hia --> .\n"
     ]
    }
   ],
   "source": [
    "# Build dataset\n",
    "block_size = 3\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for name in names[:5]:\n",
    "    print('Name:', name)\n",
    "    ctx = [0] * block_size\n",
    "    for c in name + '.':\n",
    "        idx = stoi[c]\n",
    "        X.append(ctx)\n",
    "        y.append(idx)\n",
    "        print(''.join([itos[i] for i in ctx]), '-->', itos[idx])\n",
    "        ctx = ctx[1:] + [stoi[c]]\n",
    "\n",
    "X = mx.array(X)\n",
    "y = mx.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7570295-216d-4542-acf0-ffc36e91ed45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32, 3], mlx.core.int32, [32], mlx.core.int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8883c392-d615-4413-ace9-69b7b2511ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D embedding for each letter\n",
    "C = mx.random.normal([27, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "764e2dd2-38f7-438b-8838-fac6bdcfeb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 3, 2]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed each letter in each of the examples in the training data, X\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15a36463-5cb8-4ec5-a4c1-34ff66443328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new layer\n",
    "# Input size = 3 x 2; This is b/c each example in `emb` contains 3 chars, each of which have 2 dims.\n",
    "# Output size = 100; This is arbitrary.\n",
    "W1 = mx.random.normal([6, 100])\n",
    "b1 = mx.random.normal([100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72692325-8157-490f-9c82-233193dac4ca",
   "metadata": {},
   "source": [
    "### First Layer\n",
    "We want to multiply our embedded input by our first layer of weights, add the bias, and perform a tanh function over the results to normalize the output: \n",
    "    \n",
    "    tanh(emb @ W1 + b1)\n",
    "\n",
    "\n",
    "However, the current shapes of our tensors don't support this multiplication operation:\n",
    "\n",
    "    emb.shape == [32, 3, 2]\n",
    "    W1.shape == [6, 100]\n",
    "\n",
    "\n",
    "To solve this, we need to combine the second and third dimensions of our embedded input tensor, giving us:\n",
    "    \n",
    "    emb.shape == [32, 6]\n",
    "\n",
    "\n",
    "This represents 32 examples of 3 characters, each with a 2-dimensional embedding:\n",
    "    \n",
    "    Ex. [Char1FirstEmb, Char1SecondEmb, Char2FirstEmb, Char2SecondEmb, Char3FirstEmb, Char3SecondEmb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62231fba-1ca6-4492-986a-5cfff753fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.192549, -0.398054, 0.192549, -0.398054, 0.192549, -0.398054],\n",
      "       [0.192549, -0.398054, 0.192549, -0.398054, 0.666218, -1.37186],\n",
      "       [0.192549, -0.398054, 0.666218, -1.37186, -0.304652, -0.812019],\n",
      "       [0.666218, -1.37186, -0.304652, -0.812019, -0.304652, -0.812019],\n",
      "       [-0.304652, -0.812019, -0.304652, -0.812019, -0.438571, 1.22245]], dtype=float32)\n",
      "[32, 6]\n"
     ]
    }
   ],
   "source": [
    "# We can achieve this functionality by using `reshape()`.\n",
    "# Provides a more memory-efficient way of re-shaping the array\n",
    "emb_reshaped = mx.reshape(emb, (32, 6))\n",
    "print(emb_reshaped[:5])\n",
    "print(emb_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a78ddb3-668d-4625-b79b-972e36a057e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.192549, -0.398054, 0.192549, -0.398054, 0.192549, -0.398054],\n",
      "       [0.192549, -0.398054, 0.192549, -0.398054, 0.666218, -1.37186],\n",
      "       [0.192549, -0.398054, 0.666218, -1.37186, -0.304652, -0.812019],\n",
      "       [0.666218, -1.37186, -0.304652, -0.812019, -0.304652, -0.812019],\n",
      "       [-0.304652, -0.812019, -0.304652, -0.812019, -0.438571, 1.22245]], dtype=float32)\n",
      "[32, 6]\n"
     ]
    }
   ],
   "source": [
    "# Generalize the re-shaping of the tensor to accommodate arbitrary block_size's\n",
    "emb_flattened = mx.flatten(emb, start_axis=1)\n",
    "print(emb_flattened[:5])\n",
    "print(emb_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f604eec-453f-463a-a4e0-76074e46f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.47145, -0.122259, -0.929297, ..., 0.300227, -0.859318, 0.530754],\n",
      "       [0.857721, -0.198757, -0.906659, ..., 0.0625539, -0.607978, 0.572188],\n",
      "       [0.957443, 0.398662, -0.977714, ..., -0.550226, -0.99897, 0.987817],\n",
      "       [0.975854, -0.47205, -0.988863, ..., -0.0823076, -0.939782, 0.80782],\n",
      "       [-0.7669, -0.0193273, -0.891213, ..., 0.155822, -0.991842, -0.00118141]], dtype=float32)\n",
      "[32, 100]\n"
     ]
    }
   ],
   "source": [
    "# Perform the matrix multiplication and apply tanh\n",
    "h = mx.tanh(emb_flattened @ W1 + b1)\n",
    "print(h[:5])\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abe9f7-cfff-4ce0-a5cf-d9f3a96ce144",
   "metadata": {},
   "source": [
    "### Next Layer\n",
    "This layer consists of another set of weights and biases, W2 and b2. It produces logits by multiplying the outputs of the previous layer by W2 and adding the bias vector b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "864cd4fd-3744-4a7b-a3dd-df5329df16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 27]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next layer produces logits\n",
    "W2 = mx.random.normal([100, 27])\n",
    "b2 = mx.random.normal([27])\n",
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85522a47-b6e7-47c9-8fa7-3bcdad66db13",
   "metadata": {},
   "source": [
    "### Final Layer\n",
    "To make the logits useful, we must perform a softmax operation. This gives us a vector of normalized probabilities for each character in an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "389955e3-f4f7-4357-89ba-7d02cd08bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Softmax over all logits (manually)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "784402de-a4c1-44dd-9d36-6f3477319439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 27]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "777985bf-4890-4f7f-8800-f895c114ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([5, 13, 13, ..., 9, 1, 0], dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8.61489e-10, 0.928421, 0.062959, ..., 2.21079e-08, 1.20255e-10, 3.05629e-07], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the probability of the correct character produced by the model, as defined by `Y`\n",
    "print(y)\n",
    "prob[mx.arange(32), y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce350b21-1282-4a76-8756-9102c90f97f5",
   "metadata": {},
   "source": [
    "### Calculate Loss\n",
    "With these probabilities, we are able to calculate the loss (negative log likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0da17062-259d-4efb-85e7-223e24bbc971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(15.728, dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each example, index into the y-th position to retrieve the probability calculated for the correct label.\n",
    "loss = -prob[mx.arange(32), y].log().mean()\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

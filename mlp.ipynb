{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a38fc7-a550-4bd5-a284-04313d821d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0b90f0-3b7f-4155-be85-eb0006783f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "names = []\n",
    "with open('names.txt', 'r') as file:\n",
    "    while line := file.readline():\n",
    "        names.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c850b5e-77a7-43a6-82ba-8dbbc50aceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique chars in dataset\n",
    "chars = set()\n",
    "for name in names:\n",
    "    for c in name:\n",
    "        chars.add(c)\n",
    "\n",
    "# Assign value to each character -- this will be our embedding.\n",
    "stoi = {ch:i+1 for i,ch in enumerate(sorted(chars))}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece6d689-d770-4c6a-8fa2-80dca487124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "block_size = 3\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for name in names[:5]:\n",
    "    ctx = [0] * block_size\n",
    "    for c in name + '.':\n",
    "        idx = stoi[c]\n",
    "        X.append(ctx)\n",
    "        y.append(idx)\n",
    "        ctx = ctx[1:] + [stoi[c]]\n",
    "\n",
    "X = mx.array(X)\n",
    "y = mx.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7570295-216d-4542-acf0-ffc36e91ed45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32, 3], mlx.core.int32, [32], mlx.core.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8883c392-d615-4413-ace9-69b7b2511ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D embedding for each letter\n",
    "C = mx.random.normal([27, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764e2dd2-38f7-438b-8838-fac6bdcfeb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 3, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed each letter in each of the examples in the training data, X\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a36463-5cb8-4ec5-a4c1-34ff66443328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new layer\n",
    "# Input size = 3 x 2; This is b/c each example in `emb` contains 3 chars, each of which have 2 dims.\n",
    "# Output size = 100; This is arbitrary.\n",
    "W1 = mx.random.normal([6, 100])\n",
    "b1 = mx.random.normal([100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72692325-8157-490f-9c82-233193dac4ca",
   "metadata": {},
   "source": [
    "### First Layer\n",
    "We want to multiply our embedded input by our first layer of weights, add the bias, and perform a tanh function over the results to normalize the output: \n",
    "    \n",
    "    tanh(emb @ W1 + b1)\n",
    "\n",
    "\n",
    "However, the current shapes of our tensors don't support this multiplication operation:\n",
    "\n",
    "    emb.shape == [32, 3, 2]\n",
    "    W1.shape == [6, 100]\n",
    "\n",
    "\n",
    "To solve this, we need to combine the second and third dimensions of our embedded input tensor, giving us:\n",
    "    \n",
    "    emb.shape == [32, 6]\n",
    "\n",
    "\n",
    "This represents 32 examples of 3 characters, each with a 2-dimensional embedding:\n",
    "    \n",
    "    Ex. [Char1FirstEmb, Char1SecondEmb, Char2FirstEmb, Char2SecondEmb, Char3FirstEmb, Char3SecondEmb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62231fba-1ca6-4492-986a-5cfff753fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.406554, 1.14167, 0.406554, 1.14167, 0.406554, 1.14167],\n",
      "       [0.406554, 1.14167, 0.406554, 1.14167, -0.879908, 0.838293],\n",
      "       [0.406554, 1.14167, -0.879908, 0.838293, 0.261963, 0.0893494],\n",
      "       [-0.879908, 0.838293, 0.261963, 0.0893494, 0.261963, 0.0893494],\n",
      "       [0.261963, 0.0893494, 0.261963, 0.0893494, -0.430756, -0.792048]], dtype=float32)\n",
      "[32, 6]\n"
     ]
    }
   ],
   "source": [
    "# We can achieve this functionality by using `reshape()`.\n",
    "# Provides a more memory-efficient way of re-shaping the array\n",
    "emb_reshaped = mx.reshape(emb, (32, 6))\n",
    "print(emb_reshaped[:5])\n",
    "print(emb_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a78ddb3-668d-4625-b79b-972e36a057e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.406554, 1.14167, 0.406554, 1.14167, 0.406554, 1.14167],\n",
      "       [0.406554, 1.14167, 0.406554, 1.14167, -0.879908, 0.838293],\n",
      "       [0.406554, 1.14167, -0.879908, 0.838293, 0.261963, 0.0893494],\n",
      "       [-0.879908, 0.838293, 0.261963, 0.0893494, 0.261963, 0.0893494],\n",
      "       [0.261963, 0.0893494, 0.261963, 0.0893494, -0.430756, -0.792048]], dtype=float32)\n",
      "[32, 6]\n"
     ]
    }
   ],
   "source": [
    "# Generalize the re-shaping of the tensor to accommodate arbitrary block_size's\n",
    "emb_flattened = mx.flatten(emb, start_axis=1)\n",
    "print(emb_flattened[:5])\n",
    "print(emb_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f604eec-453f-463a-a4e0-76074e46f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.99985, -0.977032, -0.937331, ..., -0.92223, -0.723898, -0.413056],\n",
      "       [-0.997375, -0.994642, -0.901601, ..., -0.962323, 0.705885, -0.556825],\n",
      "       [-0.996248, -0.825479, -0.962809, ..., -0.954416, -0.226425, -0.999833],\n",
      "       [-0.996994, -0.0197523, -0.924646, ..., -0.261878, -0.534077, -0.0220973],\n",
      "       [-0.713046, 0.602526, -0.973489, ..., 0.0509802, 0.809074, -0.944905]], dtype=float32)\n",
      "[32, 100]\n"
     ]
    }
   ],
   "source": [
    "# Perform the matrix multiplication and apply tanh\n",
    "h = mx.tanh(emb_flattened @ W1 + b1)\n",
    "print(h[:5])\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abe9f7-cfff-4ce0-a5cf-d9f3a96ce144",
   "metadata": {},
   "source": [
    "### Next Layer\n",
    "This layer consists of another set of weights and biases, W2 and b2. It produces logits by multiplying the outputs of the previous layer by W2 and adding the bias vector b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "864cd4fd-3744-4a7b-a3dd-df5329df16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 27]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next layer produces logits\n",
    "W2 = mx.random.normal([100, 27])\n",
    "b2 = mx.random.normal([27])\n",
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85522a47-b6e7-47c9-8fa7-3bcdad66db13",
   "metadata": {},
   "source": [
    "### Final Layer\n",
    "To make the logits useful, we must perform a softmax operation. This gives us a vector of normalized probabilities for each character in an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "389955e3-f4f7-4357-89ba-7d02cd08bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Softmax over all logits (manually)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784402de-a4c1-44dd-9d36-6f3477319439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 27]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777985bf-4890-4f7f-8800-f895c114ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([5, 13, 13, ..., 9, 1, 0], dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.38166e-05, 5.00042e-06, 1.00381e-07, ..., 3.27193e-08, 5.95435e-12, 2.14101e-09], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the probability of the correct character produced by the model, as defined by `Y`\n",
    "print(y)\n",
    "prob[mx.arange(32), y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce350b21-1282-4a76-8756-9102c90f97f5",
   "metadata": {},
   "source": [
    "### Calculate Loss\n",
    "With these probabilities, we are able to calculate the loss (negative log likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0da17062-259d-4efb-85e7-223e24bbc971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(13.8181, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each example, index into the y-th position to retrieve the probability calculated for the correct label.\n",
    "loss = -prob[mx.arange(32), y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506189e-14ad-42af-9a91-e6a2f4edcb4e",
   "metadata": {},
   "source": [
    "# Swap out explicit operations for library functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c97e185-ad9b-41c1-a039-65464b333aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mx.default_stream(mx.cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2b4188c-90cb-4b15-96a3-1cf8f2e8dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for word in words:\n",
    "        ctx = [0] * block_size\n",
    "        for c in word + '.':\n",
    "            idx = stoi[c]\n",
    "            X.append(ctx)\n",
    "            y.append(idx)\n",
    "            ctx = ctx[1:] + [stoi[c]]\n",
    "    \n",
    "    X = mx.array(X)\n",
    "    y = mx.array(y)\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf6f102-dcea-48a5-9598-4d328aedf9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182625, 3] [182625]\n",
      "[22655, 3] [22655]\n",
      "[22866, 3] [22866]\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "Xtr, Ytr = build_dataset(names[:n1])\n",
    "Xdev, Ydev = build_dataset(names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "726edddd-3f44-4431-9b3d-757ece6aea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of words: 32033\n",
      "# of words in training set: 25626\n",
      "# of words in dev set: 3203\n",
      "# of words in test set: 3204\n"
     ]
    }
   ],
   "source": [
    "print('Total # of words:', len(names))\n",
    "print('# of words in training set:', n1)\n",
    "print('# of words in dev set:', n2 - n1)\n",
    "print('# of words in test set:', len(names) - n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7742dbe6-cc63-4950-8e57-922f0265b69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([182625, 3], [182625])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at training dataset shape\n",
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f4f67f-5e32-4282-adca-75b537f823c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(42)\n",
    "C = mx.random.normal([27, 2])\n",
    "W1 = mx.random.normal([6, 300])\n",
    "b1 = mx.random.normal([300])\n",
    "W2 = mx.random.normal([300, 27])\n",
    "b2 = mx.random.normal([27])\n",
    "parameters = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b45cf155-9733-46f9-8ca9-48b2eab3e90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of trainable parameters\n",
    "sum(p.size for p in parameters.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "619fcb4a-0c30-47e7-8d41-f6582b6ce7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function -- passing all parameters to calculate the gradient\n",
    "# NOTE: This includes the forward pass.\n",
    "def new_loss_fn(params, ix):\n",
    "    emb_vals = mx.flatten(C[Xtr[ix]], start_axis=1)\n",
    "    h = mx.tanh(emb_vals @ params['W1'] + params['b1'])\n",
    "    logits = h @ params['W2'] + params['b2']\n",
    "    return nn.losses.cross_entropy(logits, Ytr[ix], reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d36cba1-cfb5-4059-9da9-ae25e4f25ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate loss and gradient\n",
    "loss_and_grad_fn = mx.value_and_grad(new_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f421e28b-0bc1-493e-8976-b8fed813ac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8529322147369385\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for _ in range(20000):\n",
    "    \n",
    "    # Use a minibatch\n",
    "    ix = mx.random.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # Calculate loss\n",
    "    loss, grads = loss_and_grad_fn(parameters, ix)\n",
    "\n",
    "    # Update\n",
    "    for k in parameters.keys():\n",
    "        parameters[k] += -0.01 * grads[k]\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a03d0578-f064-4c84-abe2-4649328453c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.825923204421997"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate loss on entire training set\n",
    "emb_vals = mx.flatten(C[Xtr], start_axis=1)\n",
    "h = mx.tanh(emb_vals @ parameters['W1'] + parameters['b1'])\n",
    "logits = h @ parameters['W2'] + parameters['b2']\n",
    "nn.losses.cross_entropy(logits, Ytr, reduction='mean').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77d803c-9e51-4b19-8fe3-9b59e16d0745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8314409255981445"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate loss on dev set\n",
    "emb_vals = mx.flatten(C[Xdev], start_axis=1)\n",
    "h = mx.tanh(emb_vals @ parameters['W1'] + parameters['b1'])\n",
    "logits = h @ parameters['W2'] + parameters['b2']\n",
    "nn.losses.cross_entropy(logits, Ydev, reduction='mean').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd10e60-57d2-454d-98a7-54c65cd2c9cc",
   "metadata": {},
   "source": [
    "## Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d34ac0d-a670-4379-aa53-817c9455b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kar.\n",
      "ya.\n",
      "lanei.\n",
      "naa.\n",
      "a.\n",
      "gid.\n",
      "ber.\n",
      "maryo.\n",
      "yahulr.\n",
      "anaa.\n",
      "kyrnn.\n",
      "aumlrc.\n",
      "basvymoda.\n",
      "jayeenm.\n",
      "ikhyldnn.\n",
      "syzsha.\n",
      "lan.\n",
      "jadeavaaysen.\n",
      "man.\n",
      "ter.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mx.random.seed(42 + 10)\n",
    "\n",
    "# Sample 20 names\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb_vals = mx.flatten(C[mx.array(context)])\n",
    "        h = mx.tanh(emb_vals @ parameters['W1'] + parameters['b1'])\n",
    "        logits = h @ parameters['W2'] + parameters['b2']\n",
    "        probs = mx.softmax(logits)\n",
    "        ixList = np.random.multinomial(1, probs.tolist())\n",
    "        ix = np.where(ixList == 1)[0].item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "           break\n",
    "\n",
    "    # Print result\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
